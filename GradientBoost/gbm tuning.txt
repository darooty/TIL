gbm 파라미터 튜닝
튜닝해야하는 파라미터들은 두 타입으로 구분된다. 하나는 트리 기반이고 다른 하나는 부스팅 파라미터이다.
우리가 충분한 수의 나무를 훈련한다는 것을 감안할 때, 낮은 값이 항상 더 잘 작동하기 때문에 학습률의 최적의 값은 없다 (작을수록 계속 좋아진다).

따라서, gbm은 나무의 수를 증가시킴으로써 과적합을 방지할 수 있다. 그러나 특정 학습률에서는 많은 나무가 과적합을 유발할 수 있다.
그러나 우리가 학습률을 줄이고 나무를 증가시킬 때, 계산량은 비싸고 너무 오랜 시간을 소요한다.

다음 내용을 명심해라.
1. 상대적으로 높은 학습률을 선택해라. 일반적으로 0.1이 기본이다. 그리고 0.05와 0.2 사이의 값이 사용된다.
2. 학습률에 대한 최적의 나무의 수를 결정해라. 이것은 40 ~ 70 사이에서 형성된다. 작업을 빠르게 할 수 있는 수준을 선택해라.
왜냐하면 다양한 시나리오에서 사용되고 나무의 파라미터가 결정될 것이기 때문이다.
3. 학습률과 나무의 수를 결정하기 위해 트리 기반의 파라미터들을 튜닝해라. 우리는 트리를 정의할 수 있는 서로 다른 파라미터들을 선택할 수 있다.
4. 이제 학습률을 낮게하고 나무를 증가시키면서 점점 강력한 모델을 얻게 된다.

트리기반의 파라미터들을 튜닝하기 위해 먼저 학습률과 나무의 수를 고정시켜라.
1. min_samples_split: 이것은 전체 값의 ~0.5,1% 정도로 설정한다.
2. min_samples_leaf: 직관에 따라 선택할 수 있다. 이것은 단지 과적합을 방지하기 위해서 사용된다.
3. max_depth: 관측치의 개수를 기반으로 5~8을 선택한다.
4. max_features: sqrt가 기본이다.
5. subsample: 이것은 디폴트값으로 선정한다.

ex) 
learning_rate, min_samples_split, min_samples_leaf, max_depth, max_features, subsample을 고정시킨 후 나무의 수를 그리드서치한다.
학습률 0.1을 사용했을 때 최적의 나무 수가 60(대충 중간수준)이 나올 수도 있다. 이 값은 합리적이라고 생각할 수도 있다. 하지만 항상 그런 것은 아니다.
만약 나무의 최적의 수가 범위에서 가장 낮은 값이라면 학습률을 0.05로 낮추고 다시 해본다.
반대로 나무의 최적의 수가 범위에서 가장 높은 값이라면 학습률을 키운다.

이제 트리 기반 매개 변수를 튜닝한다.
1. max_depth 및 num_samples_split을 조정합니다.
2. min_samples_leaf를 튜닝합니다.
3. max_features를 조정합니다.

튜닝 변수의 순서는 신중하게 결정해야 합니다. 결과에 더 큰 영향을 미치는 변수를 먼저 선택해야 합니다. 
예를 들어 max_depth 및 min_samples_split은 상당한 영향을 미치므로 먼저 조정하고 있습니다.
중요 참고 사항: 시스템에 따라 15분에서 30분 또는 더 많은 시간이 걸릴 수 있는 이 섹션에서 헤비듀티 그리드를 검색할 것입니다. 
시스템에서 처리할 수 있는 값에 따라 테스트하는 값의 수를 변경할 수 있습니다.
우선, 최대값 5~15를 2단계로 테스트하고 최소_샘플_split을 200~1000단계로 테스트합니다. 
이것들은 단지 제 직감에 근거한 것입니다. 더 넓은 범위를 설정한 다음 더 작은 범위에 대해 여러 번 반복을 수행할 수도 있습니다.

여기서는 30개의 조합을 실행했으며 이상적인 값은 max_depth의 경우 9개, 
min_samples_split의 경우 1000개입니다. 참고로 1000은 테스트한 극단값입니다. 최적의 값이 그 이상일 가능성이 있습니다. 
따라서 더 높은 값도 확인해야 합니다.
여기서는 max_depth 9를 최적값으로 간주하고 더 높은 min_samples_split에 대해 다른 값을 시도하지 않습니다. 
항상 최선의 방법은 아닐 수 있지만 여기서 출력을 자세히 관찰하면 대부분의 경우 최대 9가 더 효과적입니다. 또한 더 높은 min_samples_split과 함께 
30에서 70까지 5개의 min_samples_leaf 값에 대해 테스트할 수 있습니다.

여기서 min_samples_split의 경우 1200, min_samples_leaf의 경우 60의 최적 값을 얻습니다. 
또한 현재 CV점수가 0.8396점까지 올라가는 것을 볼 수 있습니다. 여기에 모델을 다시 맞추고 기능의 중요성에 대해 살펴보겠습니다.

이 모델의 변수 중요도를 기준 모델과 비교하면 이제 더 많은 변수에서 값을 도출할 수 있습니다. 또한 초기에는 일부 변수를 너무 중요시했지만 
지금은 상당히 분산되어 있습니다.
이제 마지막 트리 파라미터, 즉 max_feature를 2단계로 7에서 19까지의 7개의 값을 시도해 보겠습니다.

여기서, 우리는 최적값이 7이라는 것을 발견했는데, 이것은 제곱근이기도 합니다. 그래서 우리의 초기 가치는 최고였습니다. 
더 낮은 값을 확인하고 싶을 수 있으므로 원하는 경우 확인해야 합니다. 당분간은 7시에 머물겠습니다. 이를 통해 최종 트리 파라미터는 다음과 같습니다.
min_min_filen_filen: 1200입니다.
min_colon_leaf: 60입니다.
최대_깊이: 9입니다.
max_build: 7개입니다.

하위 샘플을 조정하고 학습률이 낮은 모델을 만듭니다.
다음 단계는 다른 하위 샘플 값을 시도하는 것입니다. 0.6,0.7,0.75,0.8,0.85,0.9 값을 살펴보겠습니다.

여기서 0.85를 최적값으로 찾았습니다. 마지막으로 필요한 모든 매개 변수를 확보했습니다. 
이제 학습률을 낮추고 그에 비례해서 평가자의 수를 늘려야 합니다. 이러한 트리는 최적의 값이 아니라 좋은 벤치마크일 수 있습니다.
트리가 증가함에 따라 CV를 수행하고 최적의 값을 찾는 데 드는 계산 비용이 점점 더 많이 들 것입니다. 모델 성능에 대한 이해를 돕기 위해 
개별 리더보드 점수를 포함시켰습니다. 데이터가 열려 있지 않기 때문에 데이터를 복제할 수는 없지만 이해하기에는 좋습니다.
학습 속도를 트리 수의 두 배(120개)로 절반으로 줄이겠습니다.

이제 원래 값의 10분의 1로 줄이겠습니다. 즉, 600그루의 경우 0.01로 줄이겠습니다.
1200 트리의 경우 0.005와 같이 원래 값의 20분의 1로 줄입니다.
여기 점수가 아주 약간 감소했음을 알 수 있습니다. 그럼 1500그루의 나무를 향해 달려보죠.
따라서 이제 프라이빗 LB 점수가 0.844점에서 0.849점으로 크게 향상되었기 때문에 이 단계가 매우 중요한 단계임을 분명히 알 수 있습니다.

여기서 사용할 수 있는 또 다른 해킹은 GBM의 'warm_start' 파라미터입니다. 이 옵션을 사용하면 항상 시작할 필요 없이 작은 단계에서 
추정기의 수를 늘리고 여러 값을 검정할 수 있습니다. 당신은 또한 내 GitHub 계정에서 이 모든 모델 코드가 포함된 iPython 노트북을 다운로드할 수 있습니다.
이 기사가 마음에 들고 XGBoost에 대한 유사한 게시물을 읽으려면 이 내용을 확인하십시오. XGBoost에서 매개 변수 튜닝에 대한 전체 가이드를 참조하십시오.





















