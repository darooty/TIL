NN Guide

1. 일반화 성능을 위해 각 층의 유닛 개수를 최대한 낮게 유지해야 한다. 여기서 낮다는 것은 훈련 데이터 사이즈에 따라 결정된다.
훈련 데이터가 많을수록 유닛 개수를 키워도 된다. 일반적으로 2의 배수를 사용한다.

2. 일반화 성능을 평가하는 방법은 교차검증을 사용하는 것이다. 훈련데이터 점수에 비해 교차데이터 점수가 많이 낮다면 과적합을 의심할 수 있다.
좀 더 디테일한 방법은 이터레이션에 따른 교차데이터 점수의 변화를 관찰하는 것이다.
예를 들어 XGB모델의 estimator를 살펴보자. estimator가 늘어날수록 최적화 성능은 향상되지만 너무 깊게 쌓으면 과적합이 발생하여 일반화 성능이 감소한다.
각 estimator의 교차점수를 확인해고 변화를 파악할 수 있다.
NN의 epoch도 XGB의 estimator와 같은 개념이다. 각 epoch에 따른 교차점수의 변화를 관찰하면 과적합 시기를 알 수 있다.

3. early stopping은 현재 모델이 과적합이 발생하기 시작할 때 epoch을 멈추는 방법이다. 주의할 것은 early stopping은 XGB의 estimator, NN의 epoch 등,
학습의 수를 조절하여 과적합을 방지하는 기법이지 한 번의 학습에서 발생하는 과적합은 막지 못한다. XGB에서 하나의 estimator의 최대 깊이, 리프노드의
최소 데이터 개수 등, 과적합을 막기 위한 다양한 시도가 존재하듯이 NN에서 은닉층의 개수, 유닛의 개수 역시 과적합을 일으킬 수 있는 파라미터이자
조절의 대상이 된다. 다만 NN은 드랍아웃, regularization 등의 몇 가지 과적합 방지 기법들이 존재한다.

4. 과적합을 방지할 수 있는 방법은 다양하지만 가장 기본적인 것은 은닉층을 적게 사용하고 각 층의 유닛을 적게 사용하는 것이다.
은닉층과 유닛 개수는 A to Z (즉 적은 수에서 점점 늘린다)로 접근한다. 너무 많은 층과 유닛은 과적합 뿐만 아니라 학습 시간도 많이 소요하기 때문이다.
많은 시행착오를 통해 좋은 파라미터를 결정한다.

5. 은닉층과 유닛 개수를 조절해나가는 과정을 신경 아키텍처 검색(NAS)라고 부른다. 이러한 하이퍼파라미터는 대부분 시행착오를 통해 선택된다.
그리드 서치, 랜덤 서치, 유전 알고리즘를 활용할 수 있다. 또, EfficientNet과 같이 정확도와 효율성을 위한 최적의 하이퍼파라미터를 갖는 네트워크를
사용해볼 수도 있다.

6. 대부분의 문제들은 깊은 은닉층을 필요로 하지 않는다. 선형으로 해결할 수 있는 문제는 하나의 층으로 해결할 수 있다(물론 몇 개의 층을 쌓아도
문제는 없다). 은닉층을 더 쌓음으로써 성능이 향상되는 경우는 극히 드물다. 하나의 은닉층에 존재하는 유닛의 개수는 대개 입력층의 크기(피처의 개수)와
출력 층의 크기(이진분류와 회귀는 1, 다중 분류는 k) 사이의 값을 많이 사용한다. 이는 많은 전문가들의 경험으로 도출된 통칙이다. 따라서 초기 은닉층의
개수는 아주 작게, 은닉층의 크기는 입력층과 출력층 사이의 값으로 설정한다.

7. 위와 같은 아이디어들은 정답은 아니다. 따라서 최적의 은닉층 개수와 크기는 직접 시행착오를 통해 얻어내야 한다. 일반적으로 특정 실제 예측 모델링
문제를 해결하기 위해 인공 신경망의 레이어 수 또는 노드의 수를 분석적으로 계산할 수 없다.
심층 신경망은 입력 변수의 공간에서 출력 변수로 증가하는 추상화 수준을 구축하는 계층을 제공한다. 주어진 문제에서 주어진 입력 공간을 사용해서
출력을 표현할 수 없을때, 즉 더 깊은 추상적인 표현이 필요하다면 심층 신경망을 사용할 수 있다(이것은 직감의 영역이다). 전문가들의 경험 상,
많은 직감들은 실험을 통해 무효화된다. 즉 실험이 가장 강력한 방법이다. 랜덤포레스트나 XGB같은 방법들로 분석을 시작하는 것이 좋다.

